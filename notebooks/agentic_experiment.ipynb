{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb384214",
   "metadata": {},
   "source": [
    "# Experiments on the design of the agentic system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0f42f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/6z_9lrtd6j3_gnlsyv9rb5b00000gn/T/ipykernel_1178/2848597642.py:29: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "# from langchain_mistralai import MistralAIEmbeddings\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "import numpy as np\n",
    "import sys\n",
    "from typing import (Annotated,Sequence,TypedDict,Any,Optional,Dict,Union,Literal)\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, ToolMessage,SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "\n",
    "load_dotenv()\n",
    "model_id = \"openai/gpt-oss-20b\" # \tqwen/qwen3-32b  openai/gpt-oss-20b llama-3.1-8b-instant\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(model=model_id,\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            verbose=1)\n",
    "\n",
    "# Create embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8a6ae",
   "metadata": {},
   "source": [
    "## Import and Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e07b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 15:26:18,821 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-12 15:26:18,883 - INFO - Going to convert document batch...\n",
      "2025-10-12 15:26:18,884 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e647edf348883bed75367b22fbe60347\n",
      "2025-10-12 15:26:18,892 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-12 15:26:18,894 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-10-12 15:26:18,904 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-12 15:26:18,907 - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-10-12 15:26:19,109 - INFO - Accelerator device: 'mps'\n",
      "2025-10-12 15:26:21,781 - INFO - Accelerator device: 'mps'\n",
      "2025-10-12 15:26:23,321 - INFO - Accelerator device: 'mps'\n",
      "2025-10-12 15:26:23,982 - INFO - Processing document suncorp_financial report 2024.pdf\n",
      "2025-10-12 15:34:10,268 - INFO - Finished converting document suncorp_financial report 2024.pdf in 471.45 sec.\n"
     ]
    }
   ],
   "source": [
    "# Load the required data and documents \n",
    "from parsing_helpers import fin_doc_parser, parse_embedding\n",
    "\n",
    "document = '../data/suncorp_financial report 2024.pdf'\n",
    "\n",
    "df_docs, docs = fin_doc_parser(document, embedding_model, min_chunk_length=800)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b51c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Suncorp df_docs\n",
    "\n",
    "df_docs.to_csv('../data/df_docs.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019ccb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the  Suncorp df_docs\n",
    "\n",
    "from parsing_helpers import  parse_embedding\n",
    "\n",
    "df_docs = pd.read_csv('../data/df_docs.csv')\n",
    "\n",
    "# Parse the 'summary_emb' column from JSON strings to numpy arrays\n",
    "df_docs[\"content_embed\"] = df_docs[\"content_embed\"].apply(parse_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd64fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doc</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>features</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_emb</th>\n",
       "      <th>dialogue_dict</th>\n",
       "      <th>conv_questions</th>\n",
       "      <th>conv_answers</th>\n",
       "      <th>turn_program</th>\n",
       "      <th>executed_answers</th>\n",
       "      <th>qa_split</th>\n",
       "      <th>conv_text_full</th>\n",
       "      <th>features_dict</th>\n",
       "      <th>num_dialogue_turns</th>\n",
       "      <th>has_type2_question</th>\n",
       "      <th>has_duplicate_columns</th>\n",
       "      <th>has_non_numeric_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Double_UPS/2009/page_33.pdf</td>\n",
       "      <td>{'pre_text': '( 1 ) includes shares repurchase...</td>\n",
       "      <td>{'conv_questions': ['what was the fluctuation ...</td>\n",
       "      <td>{'num_dialogue_turns': 7, 'has_type2_question'...</td>\n",
       "      <td>Topic: Shareowner Return Performance Graph for...</td>\n",
       "      <td>[-0.00287797605, -0.0426896326, -0.00222300575...</td>\n",
       "      <td>{'conv_questions': ['what was the fluctuation ...</td>\n",
       "      <td>['what was the fluctuation of the performance ...</td>\n",
       "      <td>['-8.94', '-8.9%', '-24.05', '-24.05%', '2.11'...</td>\n",
       "      <td>['subtract(91.06, const_100)', 'subtract(91.06...</td>\n",
       "      <td>[-8.94, -0.0894, -24.05, -0.2405, 2.11, 0.0211...</td>\n",
       "      <td>[False, False, True, True, True, True, True]</td>\n",
       "      <td>\"\"\"\\nQ: what was the fluctuation of the perfor...</td>\n",
       "      <td>{'num_dialogue_turns': 7, 'has_type2_question'...</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Double_AES/2016/page_98.pdf</td>\n",
       "      <td>{'pre_text': \"the net decrease in the 2016 eff...</td>\n",
       "      <td>{'conv_questions': ['what was the total of for...</td>\n",
       "      <td>{'num_dialogue_turns': 4, 'has_type2_question'...</td>\n",
       "      <td>Topic: \\nForeign Currency Transaction Gains an...</td>\n",
       "      <td>[-0.0433199666, -0.0162644852, -0.0239416957, ...</td>\n",
       "      <td>{'conv_questions': ['what was the total of for...</td>\n",
       "      <td>['what was the total of foreign currency trans...</td>\n",
       "      <td>['-31', '-34', '3', '-19']</td>\n",
       "      <td>['-31', '-34', 'subtract(-31, -34)', 'subtract...</td>\n",
       "      <td>[-31.0, -34.0, 3.0, -19.0]</td>\n",
       "      <td>[False, False, False, True]</td>\n",
       "      <td>\"\"\"\\nQ: what was the total of foreign currency...</td>\n",
       "      <td>{'num_dialogue_turns': 4, 'has_type2_question'...</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Double_AES/2011/page_230.pdf</td>\n",
       "      <td>{'pre_text': 'the aes corporation notes to con...</td>\n",
       "      <td>{'conv_questions': ['what is the total amount ...</td>\n",
       "      <td>{'num_dialogue_turns': 2, 'has_type2_question'...</td>\n",
       "      <td>Topic: \\nFinancial Position of AES Corporation...</td>\n",
       "      <td>[-0.0213316306, -0.0333170779, -0.0528564863, ...</td>\n",
       "      <td>{'conv_questions': ['what is the total amount ...</td>\n",
       "      <td>['what is the total amount available under the...</td>\n",
       "      <td>['2.6', '11%']</td>\n",
       "      <td>['add(1.4, 1.2)', 'divide(1697, 16088)']</td>\n",
       "      <td>[2.6, 0.10548]</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>\"\"\"\\nQ: what is the total amount available und...</td>\n",
       "      <td>{'num_dialogue_turns': 2, 'has_type2_question'...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Double_ADBE/2018/page_66.pdf</td>\n",
       "      <td>{'pre_text': 'table of contents adobe inc . no...</td>\n",
       "      <td>{'conv_questions': ['what was the weighted ave...</td>\n",
       "      <td>{'num_dialogue_turns': 3, 'has_type2_question'...</td>\n",
       "      <td>Topic: \\nGoodwill and Intangible Asset Impairm...</td>\n",
       "      <td>[-0.0828141868, -0.00688923988, -0.0301595163,...</td>\n",
       "      <td>{'conv_questions': ['what was the weighted ave...</td>\n",
       "      <td>['what was the weighted average useful life fo...</td>\n",
       "      <td>['9', '11.1%', '16.7%']</td>\n",
       "      <td>['9', 'divide(const_100, 9)', 'divide(const_10...</td>\n",
       "      <td>[9.0, 11.11111, 16.66667]</td>\n",
       "      <td>[False, False, True]</td>\n",
       "      <td>\"\"\"\\nQ: what was the weighted average useful l...</td>\n",
       "      <td>{'num_dialogue_turns': 3, 'has_type2_question'...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Double_PNC/2014/page_99.pdf</td>\n",
       "      <td>{'pre_text': '2022 timing of available informa...</td>\n",
       "      <td>{'conv_questions': ['what is the impact of tot...</td>\n",
       "      <td>{'num_dialogue_turns': 4, 'has_type2_question'...</td>\n",
       "      <td>Topic: \\nAllowance for Loan and Lease Losses (...</td>\n",
       "      <td>[-0.0417921208, 0.0188494641, -0.0804493725, 0...</td>\n",
       "      <td>{'conv_questions': ['what is the impact of tot...</td>\n",
       "      <td>['what is the impact of total net charge-offs ...</td>\n",
       "      <td>['531', '3609', '14.7%', '0.43']</td>\n",
       "      <td>['531', '3609', 'divide(531, 3609)', 'divide(2...</td>\n",
       "      <td>[531.0, 3609.0, 0.14713, 0.42457]</td>\n",
       "      <td>[False, False, False, True]</td>\n",
       "      <td>\"\"\"\\nQ: what is the impact of total net charge...</td>\n",
       "      <td>{'num_dialogue_turns': 4, 'has_type2_question'...</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "0   Double_UPS/2009/page_33.pdf   \n",
       "1   Double_AES/2016/page_98.pdf   \n",
       "2  Double_AES/2011/page_230.pdf   \n",
       "3  Double_ADBE/2018/page_66.pdf   \n",
       "4   Double_PNC/2014/page_99.pdf   \n",
       "\n",
       "                                                 doc  \\\n",
       "0  {'pre_text': '( 1 ) includes shares repurchase...   \n",
       "1  {'pre_text': \"the net decrease in the 2016 eff...   \n",
       "2  {'pre_text': 'the aes corporation notes to con...   \n",
       "3  {'pre_text': 'table of contents adobe inc . no...   \n",
       "4  {'pre_text': '2022 timing of available informa...   \n",
       "\n",
       "                                            dialogue  \\\n",
       "0  {'conv_questions': ['what was the fluctuation ...   \n",
       "1  {'conv_questions': ['what was the total of for...   \n",
       "2  {'conv_questions': ['what is the total amount ...   \n",
       "3  {'conv_questions': ['what was the weighted ave...   \n",
       "4  {'conv_questions': ['what is the impact of tot...   \n",
       "\n",
       "                                            features  \\\n",
       "0  {'num_dialogue_turns': 7, 'has_type2_question'...   \n",
       "1  {'num_dialogue_turns': 4, 'has_type2_question'...   \n",
       "2  {'num_dialogue_turns': 2, 'has_type2_question'...   \n",
       "3  {'num_dialogue_turns': 3, 'has_type2_question'...   \n",
       "4  {'num_dialogue_turns': 4, 'has_type2_question'...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Topic: Shareowner Return Performance Graph for...   \n",
       "1  Topic: \\nForeign Currency Transaction Gains an...   \n",
       "2  Topic: \\nFinancial Position of AES Corporation...   \n",
       "3  Topic: \\nGoodwill and Intangible Asset Impairm...   \n",
       "4  Topic: \\nAllowance for Loan and Lease Losses (...   \n",
       "\n",
       "                                         summary_emb  \\\n",
       "0  [-0.00287797605, -0.0426896326, -0.00222300575...   \n",
       "1  [-0.0433199666, -0.0162644852, -0.0239416957, ...   \n",
       "2  [-0.0213316306, -0.0333170779, -0.0528564863, ...   \n",
       "3  [-0.0828141868, -0.00688923988, -0.0301595163,...   \n",
       "4  [-0.0417921208, 0.0188494641, -0.0804493725, 0...   \n",
       "\n",
       "                                       dialogue_dict  \\\n",
       "0  {'conv_questions': ['what was the fluctuation ...   \n",
       "1  {'conv_questions': ['what was the total of for...   \n",
       "2  {'conv_questions': ['what is the total amount ...   \n",
       "3  {'conv_questions': ['what was the weighted ave...   \n",
       "4  {'conv_questions': ['what is the impact of tot...   \n",
       "\n",
       "                                      conv_questions  \\\n",
       "0  ['what was the fluctuation of the performance ...   \n",
       "1  ['what was the total of foreign currency trans...   \n",
       "2  ['what is the total amount available under the...   \n",
       "3  ['what was the weighted average useful life fo...   \n",
       "4  ['what is the impact of total net charge-offs ...   \n",
       "\n",
       "                                        conv_answers  \\\n",
       "0  ['-8.94', '-8.9%', '-24.05', '-24.05%', '2.11'...   \n",
       "1                         ['-31', '-34', '3', '-19']   \n",
       "2                                     ['2.6', '11%']   \n",
       "3                            ['9', '11.1%', '16.7%']   \n",
       "4                   ['531', '3609', '14.7%', '0.43']   \n",
       "\n",
       "                                        turn_program  \\\n",
       "0  ['subtract(91.06, const_100)', 'subtract(91.06...   \n",
       "1  ['-31', '-34', 'subtract(-31, -34)', 'subtract...   \n",
       "2           ['add(1.4, 1.2)', 'divide(1697, 16088)']   \n",
       "3  ['9', 'divide(const_100, 9)', 'divide(const_10...   \n",
       "4  ['531', '3609', 'divide(531, 3609)', 'divide(2...   \n",
       "\n",
       "                                    executed_answers  \\\n",
       "0  [-8.94, -0.0894, -24.05, -0.2405, 2.11, 0.0211...   \n",
       "1                         [-31.0, -34.0, 3.0, -19.0]   \n",
       "2                                     [2.6, 0.10548]   \n",
       "3                          [9.0, 11.11111, 16.66667]   \n",
       "4                  [531.0, 3609.0, 0.14713, 0.42457]   \n",
       "\n",
       "                                       qa_split  \\\n",
       "0  [False, False, True, True, True, True, True]   \n",
       "1                   [False, False, False, True]   \n",
       "2                                 [False, True]   \n",
       "3                          [False, False, True]   \n",
       "4                   [False, False, False, True]   \n",
       "\n",
       "                                      conv_text_full  \\\n",
       "0  \"\"\"\\nQ: what was the fluctuation of the perfor...   \n",
       "1  \"\"\"\\nQ: what was the total of foreign currency...   \n",
       "2  \"\"\"\\nQ: what is the total amount available und...   \n",
       "3  \"\"\"\\nQ: what was the weighted average useful l...   \n",
       "4  \"\"\"\\nQ: what is the impact of total net charge...   \n",
       "\n",
       "                                       features_dict  num_dialogue_turns  \\\n",
       "0  {'num_dialogue_turns': 7, 'has_type2_question'...                   7   \n",
       "1  {'num_dialogue_turns': 4, 'has_type2_question'...                   4   \n",
       "2  {'num_dialogue_turns': 2, 'has_type2_question'...                   2   \n",
       "3  {'num_dialogue_turns': 3, 'has_type2_question'...                   3   \n",
       "4  {'num_dialogue_turns': 4, 'has_type2_question'...                   4   \n",
       "\n",
       "   has_type2_question  has_duplicate_columns  has_non_numeric_values  \n",
       "0                True                  False                   False  \n",
       "1                True                  False                   False  \n",
       "2                True                  False                   False  \n",
       "3                True                  False                   False  \n",
       "4                True                  False                    True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the FinQA dataset\n",
    "\n",
    "from parsing_helpers import  parse_embedding\n",
    "\n",
    "\n",
    "df_finqa_0 = pd.read_csv('../data/convfinqa_full.csv')\n",
    "\n",
    "# Parse the 'summary_emb' column from JSON strings to numpy arrays\n",
    "df_finqa_0[\"summary_emb\"] = df_finqa_0[\"summary_emb\"].apply(parse_embedding)\n",
    "\n",
    "df_finqa = df_finqa_0[df_finqa_0['has_type2_question'] == True].reset_index(drop=True)\n",
    "\n",
    "print(type(df_finqa['summary_emb'].iloc[0]))\n",
    "df_finqa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ddde55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finqa_sample = df_finqa.sample(frac=0.7, random_state=42)\n",
    "df_finqa_sample.to_csv('../data/convfinqa_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd2bcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>metadata</th>\n",
       "      <th>content_embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]\\n\\n&lt;!-- image --&gt;\\n\\n[Subsection: Building ...</td>\n",
       "      <td>{'Subsection': ['Building futures and protecti...</td>\n",
       "      <td>[0.034910932183265686, -0.025240078568458557, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Subsection: Our reporting suite]\\n\\nFY24 Inve...</td>\n",
       "      <td>{'Subsection': ['Our reporting suite', 'Tax Tr...</td>\n",
       "      <td>[-0.00304177263751626, -0.01039961539208889, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Subsection: Performance highlights 1]\\n\\nGrou...</td>\n",
       "      <td>{'Subsection': [[[['Performance highlights 1',...</td>\n",
       "      <td>[0.001944231684319675, -0.0214316938072443, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Subsection: 75%]\\n\\nDigital sales up from 67%...</td>\n",
       "      <td>{'Subsection': [[['75%', '$10.4m'], '99.95%'],...</td>\n",
       "      <td>[0.025364594534039497, -0.017340196296572685, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Subsection: $1.42bn]\\n\\nEmployee salaries, su...</td>\n",
       "      <td>{'Subsection': [[['$1.42bn', '$9.7bn'], '$360m...</td>\n",
       "      <td>[-0.015178071334958076, -0.023427411913871765,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  []\\n\\n<!-- image -->\\n\\n[Subsection: Building ...   \n",
       "1  [Subsection: Our reporting suite]\\n\\nFY24 Inve...   \n",
       "2  [Subsection: Performance highlights 1]\\n\\nGrou...   \n",
       "3  [Subsection: 75%]\\n\\nDigital sales up from 67%...   \n",
       "4  [Subsection: $1.42bn]\\n\\nEmployee salaries, su...   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {'Subsection': ['Building futures and protecti...   \n",
       "1  {'Subsection': ['Our reporting suite', 'Tax Tr...   \n",
       "2  {'Subsection': [[[['Performance highlights 1',...   \n",
       "3  {'Subsection': [[['75%', '$10.4m'], '99.95%'],...   \n",
       "4  {'Subsection': [[['$1.42bn', '$9.7bn'], '$360m...   \n",
       "\n",
       "                                       content_embed  \n",
       "0  [0.034910932183265686, -0.025240078568458557, ...  \n",
       "1  [-0.00304177263751626, -0.01039961539208889, -...  \n",
       "2  [0.001944231684319675, -0.0214316938072443, -0...  \n",
       "3  [0.025364594534039497, -0.017340196296572685, ...  \n",
       "4  [-0.015178071334958076, -0.023427411913871765,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3d7b4",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9589916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (List, Annotated,Sequence,TypedDict,Any,Optional,Dict,Union,Literal)\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, ToolMessage,SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "# prompt templates\n",
    "\n",
    "\n",
    "class model_call_output(BaseModel):\n",
    "    \"\"\"Output schema for the model call.\"\"\"    \n",
    "    Thought: List[str] = Field(description= \"the AI assistant's thought process, step by step\") \n",
    "    References: Optional[List[str]] = Field(description=\"list of references the AI has used from the context to support the reasoning and final answer\")\n",
    "    Final_answer: Optional[str] = Field(description=\"the final answer to the user's question, based on the thought process and any tool results\")\n",
    "\n",
    "\n",
    "system_msg_template = \"\"\"You are a financial expert AI assistant. You have access to the following tools: {tool_names}. You are also given relevant contextual information to answer the question, together with example answers on similar questions that could help your thinking process. \n",
    "\n",
    "When you need to perform a calculation, use the appropriate tool. Always think step by step and explain your reasoning before providing an answer. Always explain your thinking process to help users understand your approach.\n",
    "\n",
    "When responding to queries:\n",
    "1. First, make sure that you respond based on the chat history and the contextual information provided by the user\n",
    "2. Use available tools if you need current data or specific capabilities  \n",
    "3. Provide clear, helpful responses based on your reasoning and any tool results\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_msg_new_template = \"\"\"\n",
    "\n",
    "# The chat history with the user is as follows delimited by CHAT_HISTORY.\n",
    "\n",
    "Chat History:\n",
    "\n",
    "<CHAT_HISTORY>\n",
    "{chat_history}\n",
    "</CHAT_HISTORY>\n",
    "\n",
    "# Question from the user: {query}\n",
    "\n",
    "# You are given the following context to help you answer the question: \n",
    "\n",
    "<CONTEXT>\n",
    "{context_content}\n",
    "</CONTEXT>\n",
    "\n",
    "# Example thinking process on similar conversational questions:\n",
    "\n",
    "<EXAMPLES>\n",
    "{context_examples}  \n",
    "</EXAMPLES>\n",
    "\n",
    "\n",
    "# You have access to the following tools, delimited by TOOLS: \n",
    "\n",
    "<TOOLS>\n",
    "{tool_names}\n",
    "</TOOLS>\n",
    "\n",
    "Please answer the user's Question based on the CHAT_HISTORY and CONTEXT. Remember to think step by step and use the TOOLS when necessary. You can inspire by the EXAMPLES provided to plan your steps.\n",
    "Provide a final answer at the end.\n",
    "If you don't know the answer, just say you don't know. Do not try to make up an answer.\n",
    "\n",
    "# If you are producing the final answer please give it in the following format (if you are going to user other tools please skip this step):\n",
    "\n",
    "Thought: [the AI assistant's thought process, step by step]\n",
    "\n",
    "References: [list of references the AI has used from the context to support the reasoning and final answer]\n",
    "\n",
    "Final_answer: [the final answer to the user's question, based on the thought process and any tool results]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_msg_tool_template = \"\"\"\n",
    "\n",
    "# Question from the user: {query}\n",
    "\n",
    "# You are given the following context to help you answer the question: \n",
    "\n",
    "<CONTEXT>\n",
    "{context_content}\n",
    "</CONTEXT>\n",
    "\n",
    "# You have access to the following tools, delimited by TOOLS: \n",
    "\n",
    "<TOOLS>\n",
    "{tool_names}\n",
    "</TOOLS>\n",
    "\n",
    "# The chat history with the user so far delimited by CHAT_HISTORY.\n",
    "\n",
    "<CHAT_HISTORY>\n",
    "{chat_history}\n",
    "</CHAT_HISTORY>\n",
    "\n",
    "Please continue answering the user's query based on the CHAT_HISTORY and CONTEXT. Remember to think step by step and use the TOOLS when necessary.\n",
    "\n",
    "\n",
    "# If you are producing the final answer please give it in the following format (if you are going to user other tools please skip this step):\n",
    "\n",
    "Thought: [the AI assistant's thought process, step by step]\n",
    "\n",
    "References: [list of references the AI has used from the context to support the reasoning and final answer]\n",
    "\n",
    "Final_answer: [the final answer to the user's question, based on the thought process and any tool results]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "system_message = SystemMessagePromptTemplate.from_template(system_msg_template)\n",
    "user_message_new = HumanMessagePromptTemplate.from_template(user_msg_new_template) \n",
    "user_message_tool = HumanMessagePromptTemplate.from_template(user_msg_tool_template) \n",
    "\n",
    "tool_flag = False\n",
    "\n",
    "if tool_flag:\n",
    "    user_message = user_message_tool\n",
    "else:\n",
    "    user_message = user_message_new\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e593f01",
   "metadata": {},
   "source": [
    "## Tools Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e467a5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add', 'subtract', 'multiply', 'divide', 'sqrt', 'power']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%writefile ../src/tools_helpers.py\n",
    "\n",
    "# math tools\n",
    "from langchain.tools import tool\n",
    "import math\n",
    "\n",
    "@tool(\"add\", return_direct=True)\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Return the sum of two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool(\"subtract\", return_direct=True)\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Return the result of subtracting b from a.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "@tool(\"multiply\", return_direct=True)\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Return the product of two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool(\"divide\", return_direct=True)\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Return the result of dividing a by b. Raises an error if dividing by zero.\"\"\"\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero.\")\n",
    "    return a / b\n",
    "\n",
    "\n",
    "@tool(\"sqrt\", return_direct=True)\n",
    "def sqrt(x: float) -> float:\n",
    "    \"\"\"Return the square root of a number. Raises an error if the input is negative.\"\"\"\n",
    "    if x < 0:\n",
    "        raise ValueError(\"Cannot take square root of a negative number.\")\n",
    "    return math.sqrt(x)\n",
    "\n",
    "\n",
    "@tool(\"power\", return_direct=True)\n",
    "def power(a: float, b: float) -> float:\n",
    "    \"\"\"Return a raised to the power of b.\"\"\"\n",
    "    return math.pow(a, b)\n",
    "\n",
    "@tool(args_schema=model_call_output)\n",
    "def model_call_tool(Thought: List[str], References:Optional[List[str]] = None, Final_answer: Optional[str] = None) :\n",
    "    \"\"\"Output the model's thought process, references and the final answer.\"\"\"\n",
    "    return {\"Thought\": Thought, \"References\": References or [],\"Final_answer\": Final_answer or \"\"}\n",
    "\n",
    "\n",
    "# output shcema tools\n",
    "\n",
    "tools=[add,subtract,multiply,divide,sqrt,power]\n",
    "\n",
    "tools_by_name={ tool.name:tool for tool in tools}\n",
    "\n",
    "tool_names=[tool.name for tool in tools]\n",
    "tool_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f877dc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Output schema for the model call.\", \"properties\": {\"Thought\": {\"description\": \"the AI assistant's thought process, step by step\", \"items\": {\"type\": \"string\"}, \"title\": \"Thought\", \"type\": \"array\"}, \"References\": {\"anyOf\": [{\"items\": {\"type\": \"string\"}, \"type\": \"array\"}, {\"type\": \"null\"}], \"description\": \"list of references the AI has used from the context to support the reasoning and final answer\", \"title\": \"References\"}, \"Final_answer\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"description\": \"the final answer to the user's question, based on the thought process and any tool results\", \"title\": \"Final Answer\"}}, \"required\": [\"Thought\", \"References\", \"Final_answer\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=model_call_output)\n",
    "\n",
    "format_instructions = parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a23057",
   "metadata": {},
   "source": [
    "## Testing the prompts and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e951e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the prompts\n",
    "from langgraph.graph.message import add_messages\n",
    "from parsing_helpers import find_relevant_document_fast, parse_embedding\n",
    "\n",
    "\n",
    "query=\"What is the net income for Suncorp insurance in 2024?\"\n",
    "\n",
    "relevant_docs = find_relevant_document_fast(embedding_model, query, df_docs, 'content_embed', top_k=2)\n",
    "relevant_finq_docs = find_relevant_document_fast(embedding_model, query, df_finqa, 'summary_emb', top_k=2)\n",
    "\n",
    "context_content = '\\n'.join([doc for doc in relevant_docs['content']])\n",
    "context_examples = '\\n'.join([f\"Example {i}: {doc}\" for i,doc in enumerate(relevant_finq_docs['conv_text_full'])]) \n",
    "\n",
    "messages = []\n",
    "queries_list = []\n",
    "messages = add_messages(messages, HumanMessage(content=query))\n",
    "queries_list.append(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "facf0e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# ----- Wrap your prompt with injected context -----\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableMap\n",
    "\n",
    "# This small wrapper injects extra variables before the prompt\n",
    "context_injector = RunnableLambda(\n",
    "    lambda inputs: {\n",
    "        \"agent_scratchpad\": inputs.get(\"agent_scratchpad\", []),\n",
    "        \"chat_history\": inputs.get(\"messages\", []),\n",
    "        \"context_content\": context_content,\n",
    "        \"context_examples\": context_examples,\n",
    "        \"query\": query,\n",
    "        \"tool_names\": \", \".join(tool_names),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Combine the injector and the prompt\n",
    "context_prompt = context_injector | chat_prompt\n",
    "\n",
    "# ----- Create the agent -----\n",
    "react_agent_executor = create_react_agent(llm, tools, prompt=context_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a065175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 16:03:39,717 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is the net income for Suncorp insurance in 2024?', additional_kwargs={}, response_metadata={}, id='2dd71596-8130-43e4-908f-3ac3a16d8737'), AIMessage(content='Thought:  \\nThe consolidated statement of comprehensive income lists the profit after tax from continuing operations, which represents the net income attributable to the insurance part of the group. For the year ended 30\\u202fJune\\u202f2024 this figure is **$971\\u202fmillion**.\\n\\nReferences:  \\n- Consolidated statement of comprehensive income – “Profit after tax from continuing operations” 2024: 971\\u202f$M.\\n\\nFinal_answer:  \\nThe net income for Suncorp Insurance in 2024 was **$971\\u202fmillion**.', additional_kwargs={'reasoning_content': 'We need net income for Suncorp insurance in 2024. The context includes consolidated statement of comprehensive income. Net income for the financial year is 1,229 million. But question: \"net income for Suncorp insurance in 2024?\" Might refer to net income attributable to Suncorp Insurance segment? The consolidated statement shows \"Profit for the financial year\" 1,229. But maybe they want net income for Suncorp Insurance (the insurance segment). In the consolidated statement, \"Insurance revenue\" etc. The net income for insurance operations? The \"Profit after tax from continuing operations\" is 971. That is profit after tax from continuing operations (excluding discontinued). But the question: \"net income for Suncorp insurance in 2024?\" Could be the net income attributable to Suncorp Insurance (the insurance business). The consolidated statement shows \"Profit after tax from continuing operations\" 971. That likely is net income for insurance operations. But the question might be ambiguous. The user asked: \"What is the net income for Suncorp insurance in 2024?\" The context: \"Suncorp insurance\" likely refers to the insurance segment. The net income for the insurance segment would be the profit after tax from continuing operations: 971 million. But the consolidated statement also shows \"Profit after tax from discontinued operation - Suncorp Bank\" 258. So net income for the insurance business is 971. But the question might also refer to \"net income\" as \"Profit after tax from continuing operations\" 971. So answer: 971 million. Provide references. Use no tool. Just answer.'}, response_metadata={'token_usage': {'completion_tokens': 450, 'prompt_tokens': 3227, 'total_tokens': 3677, 'completion_time': 0.446985855, 'prompt_time': 0.159096364, 'queue_time': 0.194704515, 'total_time': 0.606082219}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--6d1627e1-2c4a-498c-a39b-92a6f424d456-0', usage_metadata={'input_tokens': 3227, 'output_tokens': 450, 'total_tokens': 3677})]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=query)]\n",
    "\n",
    "response = react_agent_executor.invoke({\"messages\": messages})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2183c0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought:  \n",
      "The consolidated statement of comprehensive income lists the profit after tax from continuing operations, which represents the net income attributable to the insurance part of the group. For the year ended 30 June 2024 this figure is **$971 million**.\n",
      "\n",
      "References:  \n",
      "- Consolidated statement of comprehensive income – “Profit after tax from continuing operations” 2024: 971 $M.\n",
      "\n",
      "Final_answer:  \n",
      "The net income for Suncorp Insurance in 2024 was **$971 million**.\n",
      "\n",
      " ----reasoning content ---- \n",
      " \n",
      "We need net income for Suncorp insurance in 2024. The context includes consolidated statement of comprehensive income. Net income for the financial year is 1,229 million. But question: \"net income for Suncorp insurance in 2024?\" Might refer to net income attributable to Suncorp Insurance segment? The consolidated statement shows \"Profit for the financial year\" 1,229. But maybe they want net income for Suncorp Insurance (the insurance segment). In the consolidated statement, \"Insurance revenue\" etc. The net income for insurance operations? The \"Profit after tax from continuing operations\" is 971. That is profit after tax from continuing operations (excluding discontinued). But the question: \"net income for Suncorp insurance in 2024?\" Could be the net income attributable to Suncorp Insurance (the insurance business). The consolidated statement shows \"Profit after tax from continuing operations\" 971. That likely is net income for insurance operations. But the question might be ambiguous. The user asked: \"What is the net income for Suncorp insurance in 2024?\" The context: \"Suncorp insurance\" likely refers to the insurance segment. The net income for the insurance segment would be the profit after tax from continuing operations: 971 million. But the consolidated statement also shows \"Profit after tax from discontinued operation - Suncorp Bank\" 258. So net income for the insurance business is 971. But the question might also refer to \"net income\" as \"Profit after tax from continuing operations\" 971. So answer: 971 million. Provide references. Use no tool. Just answer.\n"
     ]
    }
   ],
   "source": [
    "print(response['messages'][-1].content)\n",
    "print(\"\\n ----reasoning content ---- \\n \")\n",
    "print(response['messages'][-1].additional_kwargs['reasoning_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ff3dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query=\"How much increase is it from 2023? please reply in dollar value and percentage value.\"\n",
    "\n",
    "queries_list.append(new_query)\n",
    "queries = '\\n'.join(queries_list)\n",
    "relevant_docs = find_relevant_document_fast(embedding_model, queries, df_docs, 'content_embed', top_k=2)\n",
    "relevant_finq_docs = find_relevant_document_fast(embedding_model, queries, df_finqa, 'summary_emb', top_k=2)\n",
    "\n",
    "context_content = '\\n'.join([doc for doc in relevant_docs['content']])\n",
    "context_examples = '\\n'.join([f\"Example {i}: {doc}\" for i,doc in enumerate(relevant_finq_docs['conv_text_full'])]) \n",
    "\n",
    "model_react=chat_prompt|llm.bind_tools(tools)\n",
    "\n",
    "messages = response['messages']\n",
    "messages = add_messages(messages, HumanMessage(content=new_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2820009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This small wrapper injects extra variables before the prompt\n",
    "context_injector = RunnableLambda(\n",
    "    lambda inputs: {\n",
    "        \"agent_scratchpad\": inputs.get(\"agent_scratchpad\", []),\n",
    "        \"chat_history\": inputs.get(\"messages\", []),\n",
    "        \"context_content\": context_content,\n",
    "        \"context_examples\": context_examples,\n",
    "        \"query\": new_query,\n",
    "        \"tool_names\": \", \".join(tool_names),\n",
    "    }\n",
    ")\n",
    "\n",
    "user_message = user_message_new\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message\n",
    "])\n",
    "\n",
    "# Combine the injector and the prompt\n",
    "context_prompt = context_injector | chat_prompt\n",
    "\n",
    "# ----- Create the agent -----\n",
    "react_agent_executor = create_react_agent(llm, tools, prompt=context_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "979e939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 23:49:04,709 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='How much the GWP increased with respect to FY23? ', additional_kwargs={}, response_metadata={}, id='d3a0effe-bd3b-4bc9-9bc8-52d909b81e49'), AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to answer: \"How much the GWP increased with respect to FY23?\" GWP likely stands for Gross Written Premium. We need to find GWP for FY24 and FY23. The context includes \"Insurance revenue\" in consolidated statement of comprehensive income: 13,697 for 2024, 12,081 for restated 2023. That likely is GWP? But GWP is usually gross written premium. In insurance, gross written premium is the total premium written before reinsurance. The \"Insurance revenue\" likely equals GWP. So GWP 2024 = 13,697 million, 2023 = 12,081 million. Increase = 13,697 - 12,081 = 1,616 million. Let\\'s compute.', 'tool_calls': [{'id': 'fc_0702ad06-503b-43ca-8765-a9287e356a55', 'function': {'arguments': '{\"a\":13700,\"b\":12081}', 'name': 'subtract'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 187, 'prompt_tokens': 3204, 'total_tokens': 3391, 'completion_time': 0.186460512, 'prompt_time': 0.17501404, 'queue_time': 0.194512302, 'total_time': 0.361474552}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--392a0904-bfa1-4031-b42f-4447e7bb7023-0', tool_calls=[{'name': 'subtract', 'args': {'a': 13700, 'b': 12081}, 'id': 'fc_0702ad06-503b-43ca-8765-a9287e356a55', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3204, 'output_tokens': 187, 'total_tokens': 3391}), ToolMessage(content='1619.0', name='subtract', id='ff5df611-5d76-49da-874c-977ed642e1ed', tool_call_id='fc_0702ad06-503b-43ca-8765-a9287e356a55')]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=new_query)]\n",
    "\n",
    "response = react_agent_executor.invoke({\"messages\": messages})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4237af22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1619.0\n",
      "\n",
      " ----reasoning content ---- \n",
      " \n",
      "We need to answer: \"How much the GWP increased with respect to FY23?\" GWP likely stands for Gross Written Premium. We need to find GWP for FY24 and FY23. The context includes \"Insurance revenue\" in consolidated statement of comprehensive income: 13,697 for 2024, 12,081 for restated 2023. That likely is GWP? But GWP is usually gross written premium. In insurance, gross written premium is the total premium written before reinsurance. The \"Insurance revenue\" likely equals GWP. So GWP 2024 = 13,697 million, 2023 = 12,081 million. Increase = 13,697 - 12,081 = 1,616 million. Let's compute.\n"
     ]
    }
   ],
   "source": [
    "print(response['messages'][-1].content)\n",
    "print(\"\\n ----reasoning content ---- \\n \")\n",
    "print(response['messages'][-2].additional_kwargs['reasoning_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc575247",
   "metadata": {},
   "source": [
    "## Manual implementation of REACT agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a205290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "from parsing_helpers import find_relevant_document_fast, parse_embedding\n",
    "\n",
    "\n",
    "query=\"What is the net income for Suncorp insurance in 2024?\"\n",
    "\n",
    "relevant_docs = find_relevant_document_fast(embedding_model, query, df_docs, 'content_embed', top_k=2)\n",
    "relevant_finq_docs = find_relevant_document_fast(embedding_model, query, df_finqa, 'summary_emb', top_k=2)\n",
    "\n",
    "context_content = '\\n'.join([doc for doc in relevant_docs['content']])\n",
    "context_examples = '\\n'.join([f\"Example {i}: {doc}\" for i,doc in enumerate(relevant_finq_docs['conv_text_full'])]) \n",
    "\n",
    "messages = []\n",
    "queries_list = []\n",
    "messages = add_messages(messages, HumanMessage(content=query))\n",
    "queries_list.append(query)\n",
    "\n",
    "\n",
    "user_message = user_message_new\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message\n",
    "])\n",
    "\n",
    "model_react=chat_prompt|llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c35a4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 23:50:47,579 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = model_react.invoke({\n",
    "        \"context_content\": context_content\n",
    "        , \"context_examples\": context_examples\n",
    "        , \"chat_history\": messages\n",
    "        , \"tool_names\": ', '.join(tool_names)\n",
    "        , \"query\": query\n",
    "        })\n",
    "\n",
    "while response.tool_calls and response.tool_calls[0]['name']!='model_call_tool':\n",
    "    tool_name = response.tool_calls[0]['name']\n",
    "    tool = tools_by_name[tool_name]\n",
    "    tool_call_id = response.tool_calls[0]['id']\n",
    "    tool_args = response.tool_calls[0]['args']\n",
    "    tool_response = tool.invoke(tool_args)\n",
    "    tool_message= f\"Tool {tool_name} invoked with args {tool_args}, got response: {tool_response} \\n\"\n",
    "    print(tool_message)\n",
    "    messages = add_messages(messages,ToolMessage(\n",
    "                content=tool_message,\n",
    "                name=tool_name,\n",
    "                tool_call_id=tool_call_id,\n",
    "            ) )\n",
    "    print(messages[-1])\n",
    "    user_message = user_message_tool\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        system_message,\n",
    "        user_message\n",
    "    ])\n",
    "    model_react=chat_prompt|llm.bind_tools(tools)\n",
    "\n",
    "    response = model_react.invoke({\n",
    "        \"context_content\": context_content\n",
    "        , \"context_examples\": context_examples\n",
    "        , \"chat_history\": messages\n",
    "        , \"tool_names\": ', '.join(tool_names)\n",
    "        , \"query\": query\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2045e46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The consolidated statement of comprehensive income lists the “Profit for the financial year” as the net income figure. For the year ended 30 June 2024 this amount is $1,229 million.\n",
      "\n",
      "References: Consolidated statement of comprehensive income – “Profit for the financial year” line (2024 $M).\n",
      "\n",
      "Final_answer: The net income for Suncorp (the group) in 2024 was **$1,229 million**.\n"
     ]
    }
   ],
   "source": [
    "if response.tool_calls:\n",
    "      tool_name = response.tool_calls[0]['name']\n",
    "      tool = tools_by_name[tool_name]\n",
    "      tool_call_id = response.tool_calls[0]['id']\n",
    "      tool_args = response.tool_calls[0]['args']\n",
    "      tool_response = tool.invoke(tool_args)\n",
    "      print(f\"Tool {tool_name} invoked with args {tool_args}, got response: {tool_response}\")\n",
    "      messages = add_messages(messages, ToolMessage(\n",
    "                content=json.dumps(tool_response),\n",
    "                name=tool_name,\n",
    "                tool_call_id=tool_call_id,\n",
    "            ) )\n",
    "else:\n",
    "      print(response.content)\n",
    "      messages = add_messages(messages, AIMessage(content=response.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9287bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query=\"How much increase is it from 2023? please reply in dollar value and percentage value.\"\n",
    "queries_list.append(new_query)\n",
    "queries = '\\n'.join(queries_list)\n",
    "relevant_docs = find_relevant_document_fast(embedding_model, queries, df_docs, 'content_embed', top_k=2)\n",
    "relevant_finq_docs = find_relevant_document_fast(embedding_model, queries, df_finqa, 'summary_emb', top_k=2)\n",
    "\n",
    "context_content = '\\n'.join([doc for doc in relevant_docs['content']])\n",
    "context_examples = '\\n'.join([f\"Example {i}: {doc}\" for i,doc in enumerate(relevant_finq_docs['conv_text_full'])]) \n",
    "\n",
    "user_message = user_message_new\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message\n",
    "])\n",
    "\n",
    "model_react=chat_prompt|llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "messages = add_messages(messages, HumanMessage(content=new_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a4fb769c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 23:51:02,572 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool subtract invoked with args {'a': 1229, 'b': 1082}, got response: 147.0 \n",
      "\n",
      "content=\"Tool subtract invoked with args {'a': 1229, 'b': 1082}, got response: 147.0 \\n\" name='subtract' id='51d07ac7-ebd7-4eec-b652-1f6c7d2c3c84' tool_call_id='fc_2666acaf-4dbd-494e-bb08-80c2d55a492a'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 23:51:03,253 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool divide invoked with args {'a': 147, 'b': 1082}, got response: 0.13585951940850277 \n",
      "\n",
      "content=\"Tool divide invoked with args {'a': 147, 'b': 1082}, got response: 0.13585951940850277 \\n\" name='divide' id='3f0a5cb9-882d-4e1d-8fd9-10c24f7bb843' tool_call_id='fc_0b0c0cc9-6f18-4f76-8216-dff90e92fcac'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 23:51:03,970 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = model_react.invoke({\n",
    "        \"context_content\": context_content\n",
    "        , \"context_examples\": context_examples\n",
    "        , \"chat_history\": messages\n",
    "        , \"tool_names\": ', '.join(tool_names)\n",
    "        , \"query\": new_query\n",
    "        })\n",
    "\n",
    "while response.tool_calls and response.tool_calls[0]['name']!='model_call_tool':\n",
    "    tool_name = response.tool_calls[0]['name']\n",
    "    tool = tools_by_name[tool_name]\n",
    "    tool_call_id = response.tool_calls[0]['id']\n",
    "    tool_args = response.tool_calls[0]['args']\n",
    "    tool_response = tool.invoke(tool_args)\n",
    "    tool_message= f\"Tool {tool_name} invoked with args {tool_args}, got response: {tool_response} \\n\"\n",
    "    print(tool_message)\n",
    "    messages = add_messages(messages,ToolMessage(\n",
    "                content=tool_message,\n",
    "                name=tool_name,\n",
    "                tool_call_id=tool_call_id,\n",
    "            ) )\n",
    "    print(messages[-1])\n",
    "    user_message = user_message_tool\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        system_message,\n",
    "        user_message\n",
    "    ])\n",
    "    model_react=chat_prompt|llm.bind_tools(tools)\n",
    "    \n",
    "    response = model_react.invoke({\n",
    "        \"context_content\": context_content\n",
    "        , \"context_examples\": context_examples\n",
    "        , \"chat_history\": messages\n",
    "        , \"tool_names\": ', '.join(tool_names)\n",
    "        , \"query\": new_query\n",
    "        })\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d39b6827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought:  \n",
      "- Net income (profit after income tax) for 2024 is $1,229 million (from the consolidated statement).  \n",
      "- Net income for 2023 is $1,082 million.  \n",
      "- Increase in dollar terms = 1,229 − 1,082 = 147 million.  \n",
      "- Percentage increase = (147 / 1,082) × 100 ≈ 13.59 %.  \n",
      "\n",
      "References:  \n",
      "- 2024 net income: “Segment profit (loss) after income tax” total line = **$1,229 million**.  \n",
      "- 2023 net income: “Segment profit (loss) after income tax” total line = **$1,082 million**.  \n",
      "\n",
      "Final_answer:  \n",
      "The increase from 2023 to 2024 is **$147 million**, which represents a **13.6 %** rise.\n"
     ]
    }
   ],
   "source": [
    "if response.tool_calls:\n",
    "      tool_name = response.tool_calls[0]['name']\n",
    "      tool = tools_by_name[tool_name]\n",
    "      tool_call_id = response.tool_calls[0]['id']\n",
    "      tool_args = response.tool_calls[0]['args']\n",
    "      tool_response = tool.invoke(tool_args)\n",
    "      print(f\"Tool {tool_name} invoked with args {tool_args}, got response: {tool_response}\")\n",
    "      messages = add_messages(messages, ToolMessage(\n",
    "                content=json.dumps(tool_response),\n",
    "                name=tool_name,\n",
    "                tool_call_id=tool_call_id,\n",
    "            ) )\n",
    "else:\n",
    "      print(response.content)\n",
    "      messages = add_messages(messages, AIMessage(content=response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0fb2a4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the net income for Suncorp insurance in 2024?', additional_kwargs={}, response_metadata={}, id='59546dab-f40f-4302-89af-8d159df9cb99'),\n",
       " AIMessage(content='Thought: The consolidated statement of comprehensive income lists the “Profit for the financial year” as the net income figure. For the year ended 30\\u202fJune\\u202f2024 this amount is\\u202f$1,229\\u202fmillion.\\n\\nReferences: Consolidated statement of comprehensive income – “Profit for the financial year” line (2024 $M).\\n\\nFinal_answer: The net income for Suncorp (the group) in 2024 was **$1,229\\u202fmillion**.', additional_kwargs={}, response_metadata={}, id='6b891e08-fa84-47a2-86f2-4b0c0134d126'),\n",
       " HumanMessage(content='How much increase is it from 2023? please reply in dollar value and percentage value.', additional_kwargs={}, response_metadata={}, id='f56cfd09-14a4-4025-8c4c-0561602a476a'),\n",
       " ToolMessage(content=\"Tool subtract invoked with args {'a': 1229, 'b': 1082}, got response: 147.0 \\n\", name='subtract', id='51d07ac7-ebd7-4eec-b652-1f6c7d2c3c84', tool_call_id='fc_2666acaf-4dbd-494e-bb08-80c2d55a492a'),\n",
       " ToolMessage(content=\"Tool divide invoked with args {'a': 147, 'b': 1082}, got response: 0.13585951940850277 \\n\", name='divide', id='3f0a5cb9-882d-4e1d-8fd9-10c24f7bb843', tool_call_id='fc_0b0c0cc9-6f18-4f76-8216-dff90e92fcac'),\n",
       " AIMessage(content='Thought:  \\n- Net income (profit after income tax) for 2024 is $1,229\\u202fmillion (from the consolidated statement).  \\n- Net income for 2023 is $1,082\\u202fmillion.  \\n- Increase in dollar terms = 1,229\\u202f−\\u202f1,082\\u202f=\\u202f147\\u202fmillion.  \\n- Percentage increase = (147\\u202f/\\u202f1,082)\\u202f×\\u202f100\\u202f≈\\u202f13.59\\u202f%.  \\n\\nReferences:  \\n- 2024 net income: “Segment profit (loss) after income tax” total line = **$1,229\\u202fmillion**.  \\n- 2023 net income: “Segment profit (loss) after income tax” total line = **$1,082\\u202fmillion**.  \\n\\nFinal_answer:  \\nThe increase from 2023 to 2024 is **$147\\u202fmillion**, which represents a **13.6\\u202f%** rise.', additional_kwargs={}, response_metadata={}, id='483197ff-5b6f-4144-b8de-2d14478761a9')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "95031f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query = 'How much the GWP increased with respect to FY23? '\n",
    "\n",
    "queries_list.append(new_query)\n",
    "queries = '\\n'.join(queries_list)\n",
    "relevant_docs = find_relevant_document_fast(embedding_model, queries, df_docs, 'content_embed', top_k=2)\n",
    "relevant_finq_docs = find_relevant_document_fast(embedding_model, queries, df_finqa, 'summary_emb', top_k=2)\n",
    "\n",
    "context_content = '\\n'.join([doc for doc in relevant_docs['content']])\n",
    "context_examples = '\\n'.join([f\"Example {i}: {doc}\" for i,doc in enumerate(relevant_finq_docs['conv_text_full'])]) \n",
    "\n",
    "user_message = user_message_new\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message\n",
    "])\n",
    "\n",
    "model_react=chat_prompt|llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "messages = add_messages(messages, HumanMessage(content=new_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3f1993ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 23:51:24,960 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Tool divide invoked with args {'a': 14121, 'b': 1.139}, got response: 12397.717295873574 \\n\" name='divide' id='59db1794-ff66-4d46-9457-96268249d056' tool_call_id='fc_0b68c24a-a4ff-4459-bb9d-f42890acc99f'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 23:51:25,815 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Tool subtract invoked with args {'a': 14121, 'b': 12397.717295873574}, got response: 1723.2827041264263 \\n\" name='subtract' id='a542088f-249e-4908-8bd8-1358a793e75c' tool_call_id='fc_bb57f454-6e92-4a94-a8d2-8df5ffc3c014'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 23:51:26,839 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = model_react.invoke({\n",
    "        \"context_content\": context_content\n",
    "        , \"context_examples\": context_examples\n",
    "        , \"chat_history\": messages\n",
    "        , \"tool_names\": ', '.join(tool_names)\n",
    "        , \"query\": new_query\n",
    "        })\n",
    "\n",
    "while response.tool_calls and response.tool_calls[0]['name']!='model_call_tool':\n",
    "    tool_name = response.tool_calls[0]['name']\n",
    "    tool = tools_by_name[tool_name]\n",
    "    tool_call_id = response.tool_calls[0]['id']\n",
    "    tool_args = response.tool_calls[0]['args']\n",
    "    tool_response = tool.invoke(tool_args)\n",
    "    tool_message= f\"Tool {tool_name} invoked with args {tool_args}, got response: {tool_response} \\n\"\n",
    "    messages = add_messages(messages,ToolMessage(\n",
    "                content=tool_message,\n",
    "                name=tool_name,\n",
    "                tool_call_id=tool_call_id,\n",
    "            ) )\n",
    "    print(messages[-1])\n",
    "    user_message = user_message_tool\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        system_message,\n",
    "        user_message\n",
    "    ])\n",
    "    model_react=chat_prompt|llm.bind_tools(tools)\n",
    "    \n",
    "    response = model_react.invoke({\n",
    "        \"context_content\": context_content\n",
    "        , \"context_examples\": context_examples\n",
    "        , \"chat_history\": messages\n",
    "        , \"tool_names\": ', '.join(tool_names)\n",
    "        , \"query\": new_query\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "00a01664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Thought:**  \n",
      "- FY24 General Insurance GWP is given as **$14,121 m**.  \n",
      "- The report states that GWP increased by **13.9 %** from FY23 to FY24.  \n",
      "- To find FY23 GWP:  \n",
      "  \\[\n",
      "  \\text{FY23 GWP} = \\frac{14{,}121}{1.139} \\approx 12{,}397.72\\text{ m}\n",
      "  \\]  \n",
      "  (calculated with the divide tool).  \n",
      "- Dollar increase:  \n",
      "  \\[\n",
      "  14{,}121 - 12{,}397.72 \\approx 1{,}723.28\\text{ m}\n",
      "  \\]  \n",
      "  (calculated with the subtract tool).  \n",
      "- Percentage increase is the stated 13.9 %.\n",
      "\n",
      "**References:**  \n",
      "- “Gross Written Premium (GWP) … increased by 13.9% across the General Insurance business” (context).  \n",
      "- FY24 GWP figure of $14,121 m (context).\n",
      "\n",
      "**Final_answer:**  \n",
      "The General Insurance GWP rose by **$1.72 billion** from FY23 to FY24, representing a **13.9 %** increase.\n"
     ]
    }
   ],
   "source": [
    "if response.tool_calls:\n",
    "      tool_name = response.tool_calls[0]['name']\n",
    "      tool = tools_by_name[tool_name]\n",
    "      tool_call_id = response.tool_calls[0]['id']\n",
    "      tool_args = response.tool_calls[0]['args']\n",
    "      tool_response = tool.invoke(tool_args)\n",
    "      print(f\"Tool {tool_name} invoked with args {tool_args}, got response: {tool_response}\")\n",
    "      messages = add_messages(messages, ToolMessage(\n",
    "                content=json.dumps(tool_response),\n",
    "                name=tool_name,\n",
    "                tool_call_id=tool_call_id,\n",
    "            ) )\n",
    "else:\n",
    "      print(response.content)\n",
    "      messages = add_messages(messages, AIMessage(content=response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dadd365",
   "metadata": {},
   "source": [
    "## develop the graph workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b21dbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "# from langchain_mistralai import MistralAIEmbeddings\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "from typing import  (List, Annotated,Sequence,TypedDict,Any,Optional,Dict,Union,Literal)\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, ToolMessage,SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "from parsing_helpers import find_relevant_document_fast, parse_embedding\n",
    "from tool_helpers import tools, tools_by_name, tool_names\n",
    "from prompt_helpers import system_message, user_message_new, user_message_tool\n",
    "\n",
    "\n",
    "# Load the FinQA dataset\n",
    "\n",
    "df_finqa_0 = pd.read_csv('../data/convfinqa_full.csv')\n",
    "\n",
    "# Parse the 'summary_emb' column from JSON strings to numpy arrays\n",
    "df_finqa_0[\"summary_emb\"] = df_finqa_0[\"summary_emb\"].apply(parse_embedding)\n",
    "\n",
    "df_finqa = df_finqa_0[df_finqa_0['has_type2_question'] == True].reset_index(drop=True)\n",
    "\n",
    "\n",
    "class FinState(MessagesState):\n",
    "    \"\"\" The state of the AI Assistant\"\"\"\n",
    "    report_state: Annotated[bool, \"Whether the financial report is provided by the user\"]\n",
    "    user_query: Annotated[List[str], \"The list of questions from the user\"]\n",
    "    next_node: Annotated[Optional[str], \"The next node to execute, either 'tool_node' or 'call_model_node'\"]\n",
    "    call_mode: Annotated[Literal['user','tool_call'], \"The current mode of the call, either 'user' for model calls or 'tool_call' for tool executions\"]\n",
    "    context_content: Annotated[str, \"The relevant content from the financial report to assist in answering the user's query\"]\n",
    "    context_examples: Annotated[Optional[str], \"The relevant example conversations from the FinQA dataset to assist in answering the user's query\"]\n",
    "    message_out: Annotated[Optional[str], \"The final answer to the user's query, if available\"]\n",
    "    full_response: Annotated[Optional[Any], \"The full response from the model, including any tool calls or reasoning steps\"]\n",
    "\n",
    "def check_info_node(state: FinState):\n",
    "    \"\"\"Check if the financial report is provided by the user\"\"\"\n",
    "    # Simple heuristic: if context_docs is empty or too short, we consider it insufficient\n",
    "    if not state['report_state']:\n",
    "        \n",
    "        state['next_node'] = \"END\"\n",
    "        state['message_out'] = \"I'm sorry, but I don't have enough information to answer your question. Please provide the necessary financial documents.\"\n",
    "    else:\n",
    "        state['next_node'] = \"call_model_node\"\n",
    "    return {**state,\n",
    "            'message_out': state['message_out'],\n",
    "            'next_node': state['next_node'],\n",
    "            \"messages\": []}\n",
    "\n",
    "def should_continue_chat(state: FinState):\n",
    "    \"\"\"Determine whether to continue the chat or end the cycle.\"\"\"\n",
    "    # If there is no function call, then we finish\n",
    "    if state['next_node']==\"END\":\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "def tool_node(state: FinState):\n",
    "    \"\"\"Execute all tool calls from the last message in the state.\"\"\"\n",
    "    outputs = []\n",
    "    for tool_call in state[\"full_response\"].tool_calls:\n",
    "        tool_name = tool_call['name']\n",
    "        tool = tools_by_name[tool_name]\n",
    "        tool_call_id = tool_call['id']\n",
    "        tool_args = tool_call['args']\n",
    "        tool_response = tool.invoke(tool_args)\n",
    "        tool_message= f\"Tool {tool_name} invoked with args {tool_args}, got response: {tool_response} \\n\"\n",
    "        print(tool_message)\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=tool_message,\n",
    "                name=tool_name,\n",
    "                tool_call_id=tool_call_id,\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "def call_model_node(state: FinState):\n",
    "    \"\"\"Invoke the model with the current conversation state.\"\"\"\n",
    "    if state['call_mode']=='user':\n",
    "        query = state['user_query'][-1]\n",
    "        queries = '\\n'.join(state['user_query'])\n",
    "        relevant_docs = find_relevant_document_fast(embedding_model, queries, df_docs, 'content_embed', top_k=3)\n",
    "        relevant_finq_docs = find_relevant_document_fast(embedding_model, queries, df_finqa, 'summary_emb', top_k=2)\n",
    "\n",
    "        context_content = '\\n'.join([doc for doc in relevant_docs['content']])\n",
    "        context_examples = '\\n'.join([f\"Example {i}: {doc}\" for i,doc in enumerate(relevant_finq_docs['conv_text_full'])]) \n",
    "        state['context_content'] = context_content\n",
    "        state['context_examples'] = context_examples\n",
    "\n",
    "        user_message = user_message_new\n",
    "    else:\n",
    "        user_message = user_message_tool\n",
    "        query = state['user_query'][-1]\n",
    "        context_content = state['context_content']\n",
    "        context_examples = state['context_examples']\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        system_message,\n",
    "        user_message\n",
    "    ])\n",
    "\n",
    "    model_react=chat_prompt|llm.bind_tools(tools)\n",
    "\n",
    "    response = model_react.invoke({\n",
    "        \"context_content\": context_content\n",
    "        , \"context_examples\": context_examples\n",
    "        , \"chat_history\": state[\"messages\"]\n",
    "        , \"tool_names\": ', '.join(tool_names)\n",
    "        , \"query\": query\n",
    "        })\n",
    "    \n",
    "    state['full_response'] = response\n",
    "    if not response.tool_calls:\n",
    "        state['call_mode'] = 'user'\n",
    "        state['message_out'] = response.content\n",
    "        out_message = [response.content]\n",
    "    else:\n",
    "        state['call_mode'] = 'tool_call'\n",
    "        print(response.tool_calls)\n",
    "        out_message = []\n",
    "        \n",
    "    \n",
    "\n",
    "    return {**state,\n",
    "            'call_mode': state['call_mode'],\n",
    "            'context_content': state['context_content'],\n",
    "            'context_examples': state['context_examples'],\n",
    "            'message_out': state['message_out'],\n",
    "            'full_response': state['full_response'],\n",
    "            \"messages\": out_message}\n",
    "\n",
    "\n",
    "def should_continue_react(state: FinState):\n",
    "    \"\"\"Determine whether to continue with tool use or end the ReAct loop.\"\"\"\n",
    "    call_mode = state['call_mode']\n",
    "    # If there is no function call, then we finish\n",
    "    if call_mode=='user':\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "    \n",
    "\n",
    "builder = StateGraph(FinState)\n",
    "builder.add_node(\"check_info_node\", check_info_node)\n",
    "builder.add_node(\"call_model_node\", call_model_node)\n",
    "builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "builder.add_edge(START, \"check_info_node\")\n",
    "builder.add_conditional_edges(\"check_info_node\",\n",
    "                              should_continue_chat, \n",
    "                              {\"continue\": \"call_model_node\",\n",
    "                               \"end\": END})\n",
    "builder.add_conditional_edges(\"call_model_node\", \n",
    "                              should_continue_react, \n",
    "                              {\"continue\": \"tool_node\",\n",
    "                               \"end\": END})\n",
    "builder.add_edge(\"tool_node\", \"call_model_node\")\n",
    "\n",
    "finqa_graph = builder.compile(checkpointer=InMemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4f64173c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the graph\n",
    "from langchain_core.runnables.graph_mermaid import draw_mermaid_png\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(finqa_graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2003f922",
   "metadata": {},
   "source": [
    "## Test the Graph Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786144e",
   "metadata": {},
   "source": [
    "Question:\n",
    "By what percentage did Net Profit After Tax (NPAT) increase from FY23 to FY24?\n",
    "\n",
    "Answer: NPAT increased by 11.76% from FY23 to FY24.\n",
    "\n",
    "Question:\n",
    "By how many million dollars did net investment returns rise in FY24, and what is the percentage increase?\n",
    "\n",
    "Answer: Investment returns rose by A$210m, a 46.56% increase.\n",
    "\n",
    "Question:\n",
    "What proportion of cash earnings did net incurred claims represent in FY24?\n",
    "\n",
    "Answer: Net incurred claims were 361.1% of cash earnings.\n",
    "\n",
    "Question:\n",
    "If Suncorp paid out 72.1% of its cash earnings as dividends in FY24, what was the total dividend amount?\n",
    "\n",
    "Answer: Total dividends ≈ A$988.3 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2c307c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the  Suncorp df_docs\n",
    "\n",
    "from parsing_helpers import  parse_embedding\n",
    "\n",
    "df_docs = pd.read_csv('../data/df_docs.csv')\n",
    "\n",
    "# Parse the 'summary_emb' column from JSON strings to numpy arrays\n",
    "df_docs[\"content_embed\"] = df_docs[\"content_embed\"].apply(parse_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "094e15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_state = FinState(  \n",
    "    messages=[],\n",
    "    report_state=True,\n",
    "    user_query=[],\n",
    "    next_node=None,\n",
    "    call_mode='user',\n",
    "    context_content=\"\",\n",
    "    context_examples=\"\",\n",
    "    message_out=None\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "066b4c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 01:19:30,333 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'divide', 'args': {'a': 115, 'b': 1082}, 'id': 'fc_75dab880-4fac-4b42-9ef6-abb23d97687e', 'type': 'tool_call'}]\n",
      "Tool divide invoked with args {'a': 115, 'b': 1082}, got response: 0.10628465804066543 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 01:19:31,871 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Thought:**  \n",
      "1. The context states that Suncorp’s net profit after tax (NPAT) for FY24 was **$1,197 million**.  \n",
      "2. For FY23 we can calculate NPAT from the figures given:  \n",
      "   * Profit before income tax FY23 = **$1,541 million**  \n",
      "   * Income tax expense FY23 = **$459 million**  \n",
      "   * NPAT FY23 = 1,541 − 459 = **$1,082 million**.  \n",
      "3. The dollar increase = 1,197 − 1,082 = **$115 million**.  \n",
      "4. Percentage increase = 115 ÷ 1,082 ≈ **0.1063** → **10.6 %**.  \n",
      "   (The divide tool was used to confirm this calculation.)\n",
      "\n",
      "**References:**  \n",
      "- Net profit after tax FY24: $1,197 million (context).  \n",
      "- Profit before income tax FY23: $1,541 million (context).  \n",
      "- Income tax expense FY23: $459 million (context).  \n",
      "\n",
      "**Final_answer:**  \n",
      "Net Profit After Tax (NPAT) increased by **about 10.6 %** from FY23 to FY24.\n"
     ]
    }
   ],
   "source": [
    "user_id = 'reza'\n",
    "query=\"By what percentage did Net Profit After Tax (NPAT) increase from FY23 to FY24?\"\n",
    "state=initial_state\n",
    "state['messages'] = add_messages(initial_state['messages'], HumanMessage(content=query))\n",
    "state['user_query'].append(query)\n",
    "\n",
    "run_config = {'configurable': {'thread_id':user_id}}\n",
    "\n",
    "response = finqa_graph.invoke(state, config= run_config)\n",
    "print(response['message_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d38235be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 01:20:01,873 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'subtract', 'args': {'a': 661, 'b': 451}, 'id': 'fc_7c520ab1-1afc-4248-ba10-bd98216d7112', 'type': 'tool_call'}]\n",
      "Tool subtract invoked with args {'a': 661, 'b': 451}, got response: 210.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 01:20:02,691 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'divide', 'args': {'a': 210, 'b': 451}, 'id': 'fc_5c30505a-304c-4c66-98ec-668df02db0b3', 'type': 'tool_call'}]\n",
      "Tool divide invoked with args {'a': 210, 'b': 451}, got response: 0.4656319290465632 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 01:20:03,817 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Thought:**  \n",
      "The context states that net investment returns rose from **$451 million** to **$661 million** in FY24.  \n",
      "1. **Dollar increase**: \\(661 - 451 = 210\\) million dollars (calculated with the `subtract` tool).  \n",
      "2. **Percentage increase**: \\(\\frac{210}{451} \\approx 0.4656\\). Multiply by 100 to express as a percent: \\(0.4656 \\times 100 \\approx 46.6\\%\\) (calculated with the `divide` tool).  \n",
      "\n",
      "**References:**  \n",
      "- “Net investment returns were again a key contributor to the topline earnings and profit, up significantly from $451 million to $661 million.” (Context)\n",
      "\n",
      "**Final_answer:**  \n",
      "Net investment returns rose by **$210 million** in FY24, which represents a **~46.6 %** increase.\n"
     ]
    }
   ],
   "source": [
    "query=\"By how many million dollars did net investment returns rise in FY24, and what is the percentage increase?\"\n",
    "\n",
    "state['messages'] = add_messages(initial_state['messages'], HumanMessage(content=query))\n",
    "state['user_query'].append(query)\n",
    "\n",
    "run_config = {'configurable': {'thread_id':user_id}}\n",
    "\n",
    "response = finqa_graph.invoke(state, config= run_config)\n",
    "print(response['message_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f3a1a41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 01:22:15,199 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'a': 1372, 'b': 0.721}, 'id': 'fc_b8bb0ead-33e2-4b46-9290-04fd68a6c664', 'type': 'tool_call'}]\n",
      "Tool multiply invoked with args {'a': 1372, 'b': 0.721}, got response: 989.212 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 01:22:15,929 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Thought:**  \n",
      "- Cash earnings for FY24 are $1,372 million (from the context).  \n",
      "- Dividends paid were 72.1 % of cash earnings.  \n",
      "- Calculate 0.721 × 1,372 million = 989.212 million.  \n",
      "\n",
      "**References:**  \n",
      "- “cash earnings increased 16.6% to $1,372 million.”  \n",
      "- “The Group's full year dividend payout ratio of 72.1% of cash earnings…”  \n",
      "\n",
      "**Final_answer:**  \n",
      "Suncorp paid out approximately **$989.2 million** in dividends in FY24.\n"
     ]
    }
   ],
   "source": [
    "query =\" If Suncorp paid out 72.1% of its cash earnings as dividends in FY24, what was the total dividend amount?\"\n",
    "\n",
    "state['messages'] = add_messages(initial_state['messages'], HumanMessage(content=query))\n",
    "state['user_query'].append(query)\n",
    "\n",
    "run_config = {'configurable': {'thread_id':user_id}}\n",
    "\n",
    "response = finqa_graph.invoke(state, config= run_config)\n",
    "print(response['message_out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6022a",
   "metadata": {},
   "source": [
    "## Write the agentic workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02107507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the  Suncorp df_docs\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "from parsing_helpers import  parse_embedding\n",
    "\n",
    "df_docs = pd.read_csv('../data/df_docs.csv')\n",
    "\n",
    "# Parse the 'summary_emb' column from JSON strings to numpy arrays\n",
    "df_docs[\"content_embed\"] = df_docs[\"content_embed\"].apply(parse_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "439b1e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/agentic_system.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile ../src/agentic_system.py\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "# from langchain_mistralai import MistralAIEmbeddings\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from typing import  (List, Annotated,Sequence,TypedDict,Any,Optional,Dict,Union,Literal)\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, ToolMessage,SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "from parsing_helpers import find_relevant_document_fast, parse_embedding\n",
    "from tool_helpers import tools, tools_by_name, tool_names\n",
    "from prompt_helpers import system_message, user_message_new, user_message_tool\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "# Load the FinQA dataset\n",
    "\n",
    "df_finqa_0 = pd.read_csv('../data/convfinqa_full.csv')\n",
    "\n",
    "# Parse the 'summary_emb' column from JSON strings to numpy arrays\n",
    "df_finqa_0[\"summary_emb\"] = df_finqa_0[\"summary_emb\"].apply(parse_embedding)\n",
    "\n",
    "df_finqa = df_finqa_0[df_finqa_0['has_type2_question'] == True].reset_index(drop=True)\n",
    "\n",
    "\n",
    "model_id = \"openai/gpt-oss-20b\" # \tqwen/qwen3-32b  openai/gpt-oss-20b llama-3.1-8b-instant\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(model=model_id,\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            verbose=1)\n",
    "\n",
    "# Create embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "\n",
    "\n",
    "class FinState(MessagesState):\n",
    "    \"\"\" The state of the AI Assistant\"\"\"\n",
    "    report_state: Annotated[bool, \"Whether the financial report is provided by the user\"]\n",
    "    user_query: Annotated[List[str], \"The list of questions from the user in continuous chat\"]\n",
    "    next_node: Annotated[Optional[str], \"The next node to execute, either 'tool_node' or 'call_model_node'\"]\n",
    "    call_mode: Annotated[Literal['user','tool_call'], \"The current mode of the call, either 'user' for model calls or 'tool_call' for tool executions\"]\n",
    "    context_content: Annotated[str, \"The relevant content from the financial report to assist in answering the user's query\"]\n",
    "    context_examples: Annotated[Optional[str], \"The relevant example conversations from the FinQA dataset to assist in answering the user's query\"]\n",
    "    message_out: Annotated[Optional[str], \"The final answer to the user's query, if available\"]\n",
    "    full_response: Annotated[Optional[Any], \"The full response from the model, including any tool calls or reasoning steps\"]\n",
    "    query_cont: Annotated[List[str], \"Whether this is a new query or a continuation of the previous ones\"]\n",
    "\n",
    "def check_info_node(state: FinState):\n",
    "    \"\"\"Check if the financial report is provided by the user\"\"\"\n",
    "    # Simple heuristic: if context_docs is empty or too short, we consider it insufficient\n",
    "    if not state['report_state']:\n",
    "\n",
    "        state['next_node'] = \"END\"\n",
    "        state['message_out'] = \"I'm sorry, but I don't have enough information to answer your question. Please provide the necessary financial documents.\"\n",
    "    else:\n",
    "        state['next_node'] = \"call_model_node\"\n",
    "    \n",
    "    # chekc if this is a new query or continuation\n",
    "    prompt_template = PromptTemplate.from_template(\"This is the new query by the user: {new_query}. This is the previous queries by the user: {prev_queries}. Is the new query a continuation of the previous queries or a new one? Only Answer Yes or No.\")\n",
    "\n",
    "    new_query = state['user_query'][-1]\n",
    "    prev_queries = '\\n'.join(state['user_query'][:-1])\n",
    "\n",
    "    chain = prompt_template|llm\n",
    "    response = chain.invoke({\"new_query\": new_query,\n",
    "                                \"prev_queries\": '\\n'.join(prev_queries)})\n",
    "    \n",
    "    print('A continuation Question? ', response.content)\n",
    "    if 'yes' in response.content.lower():\n",
    "        state['query_cont'].append(new_query)\n",
    "    else:\n",
    "        state['query_cont'] = [new_query]\n",
    "\n",
    "    print('connected queries: \\n ', state['query_cont'])\n",
    "    return {**state,\n",
    "            'message_out': state['message_out'],\n",
    "            'next_node': state['next_node'],\n",
    "            'query_cont': state['query_cont'],\n",
    "            \"messages\": []}\n",
    "\n",
    "def should_continue_chat(state: FinState):\n",
    "    \"\"\"Determine whether to continue the chat or end the cycle.\"\"\"\n",
    "    # If there is no function call, then we finish\n",
    "    if state['next_node']==\"END\":\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "def tool_node(state: FinState):\n",
    "    \"\"\"Execute all tool calls from the last message in the state.\"\"\"\n",
    "    outputs = []\n",
    "    for tool_call in state[\"full_response\"].tool_calls:\n",
    "        tool_name = tool_call['name']\n",
    "        tool = tools_by_name[tool_name]\n",
    "        tool_call_id = tool_call['id']\n",
    "        tool_args = tool_call['args']\n",
    "        tool_response = tool.invoke(tool_args)\n",
    "        tool_message= f\"Tool {tool_name} invoked with args {tool_args}, got response: {tool_response} \\n\"\n",
    "        print(tool_message)\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=tool_message,\n",
    "                name=tool_name,\n",
    "                tool_call_id=tool_call_id,\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "def call_model_node(state: FinState):\n",
    "    \"\"\"Invoke the model with the current conversation state.\"\"\"\n",
    "    if state['call_mode']=='user':\n",
    "        query = state['user_query'][-1]\n",
    "        queries = '\\n'.join(state['query_cont'])\n",
    "        relevant_docs = find_relevant_document_fast(embedding_model, queries, df_docs, 'content_embed', top_k=4)\n",
    "        relevant_finq_docs = find_relevant_document_fast(embedding_model, queries, df_finqa, 'summary_emb', top_k=2)\n",
    "\n",
    "        context_content = '\\n'.join([doc for doc in relevant_docs['content']])\n",
    "        context_examples = '\\n'.join([f\"Example {i}: {doc}\" for i,doc in enumerate(relevant_finq_docs['conv_text_full'])]) \n",
    "        state['context_content'] = context_content\n",
    "        state['context_examples'] = context_examples\n",
    "\n",
    "        user_message = user_message_new\n",
    "    else:\n",
    "        user_message = user_message_tool\n",
    "        query = state['user_query'][-1]\n",
    "        context_content = state['context_content']\n",
    "        context_examples = state['context_examples']\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        system_message,\n",
    "        user_message\n",
    "    ])\n",
    "\n",
    "    model_react=chat_prompt|llm.bind_tools(tools)\n",
    "\n",
    "    response = model_react.invoke({\n",
    "        \"context_content\": context_content\n",
    "        , \"context_examples\": context_examples\n",
    "        , \"chat_history\": state[\"messages\"]\n",
    "        , \"tool_names\": ', '.join(tool_names)\n",
    "        , \"query\": query\n",
    "        })\n",
    "\n",
    "    state['full_response'] = response\n",
    "    if not response.tool_calls:\n",
    "        state['call_mode'] = 'user'\n",
    "        state['message_out'] = response.content\n",
    "        out_message = [response.content]\n",
    "    else:\n",
    "        state['call_mode'] = 'tool_call'\n",
    "        print(response.tool_calls)\n",
    "        out_message = []\n",
    "\n",
    "\n",
    "\n",
    "    return {**state,\n",
    "            'call_mode': state['call_mode'],\n",
    "            'context_content': state['context_content'],\n",
    "            'context_examples': state['context_examples'],\n",
    "            'message_out': state['message_out'],\n",
    "            'full_response': state['full_response'],\n",
    "            \"messages\": out_message}\n",
    "\n",
    "\n",
    "def should_continue_react(state: FinState):\n",
    "    \"\"\"Determine whether to continue with tool use or end the ReAct loop.\"\"\"\n",
    "    call_mode = state['call_mode']\n",
    "    # If there is no function call, then we finish\n",
    "    if call_mode=='user':\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "builder = StateGraph(FinState)\n",
    "builder.add_node(\"check_info_node\", check_info_node)\n",
    "builder.add_node(\"call_model_node\", call_model_node)\n",
    "builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "builder.add_edge(START, \"check_info_node\")\n",
    "builder.add_conditional_edges(\"check_info_node\",\n",
    "                              should_continue_chat, \n",
    "                              {\"continue\": \"call_model_node\",\n",
    "                               \"end\": END})\n",
    "builder.add_conditional_edges(\"call_model_node\", \n",
    "                              should_continue_react, \n",
    "                              {\"continue\": \"tool_node\",\n",
    "                               \"end\": END})\n",
    "builder.add_edge(\"tool_node\", \"call_model_node\")\n",
    "\n",
    "finqa_graph = builder.compile(checkpointer=InMemorySaver())\n",
    "\n",
    "\n",
    "\n",
    "class FinQAResponseAgent:\n",
    "    \"\"\"Async wrapper for running the finqa_graph ReAct agent as a reusable service.\"\"\"\n",
    "\n",
    "    def __init__(self, agent_graph, report_state: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the response agent.\n",
    "\n",
    "        Args:\n",
    "            agent_graph: Compiled LangGraph instance (e.g., finqa_graph)\n",
    "            report_state: Whether the financial report is provided by the user\n",
    "        \"\"\"\n",
    "        self.agent = agent_graph\n",
    "        self.report_state = report_state\n",
    "        self.df_docs = None  # To be set after PDF upload\n",
    "\n",
    "    def set_report_state(self, value: bool):\n",
    "        \"\"\"Update report_state dynamically (e.g., after PDF upload).\"\"\"\n",
    "        self.report_state = value\n",
    "    \n",
    "    def set_documents(self, df_docs):\n",
    "        \"\"\"Attach parsed financial report (and optionally embedding model) to the agent.\"\"\"\n",
    "        self.df_docs = df_docs\n",
    "\n",
    "    def convert_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert user input to a format acceptable by finqa_graph.\n",
    "\n",
    "        Args:\n",
    "            input_data (dict): {'user_id': ..., 'user_input': ...}\n",
    "\n",
    "        Returns:\n",
    "            dict: Input state for the LangGraph.\n",
    "        \"\"\"\n",
    "        user_input = input_data.get(\"user_input\", \"\")\n",
    "        user_id = input_data.get(\"user_id\", \"default_user\")\n",
    "\n",
    "        # Build run config for this user\n",
    "        run_config = {\"configurable\": {\"thread_id\": user_id}}\n",
    "\n",
    "        # Check if a state already exists (for continuity)\n",
    "        existing_state = self.agent.get_state(config=run_config)\n",
    "\n",
    "        # Build initial state only if needed\n",
    "        if existing_state is None or not existing_state.values:\n",
    "            state = {\n",
    "                \"messages\": [],\n",
    "                \"report_state\": self.report_state,  \n",
    "                \"user_query\": [user_input],\n",
    "                \"next_node\": None,\n",
    "                \"call_mode\": \"user\",\n",
    "                \"context_content\": \"\",\n",
    "                \"context_examples\": \"\",\n",
    "                \"message_out\": None,\n",
    "                \"full_response\": None,\n",
    "            }\n",
    "            state['messages'] = add_messages(state['messages'], HumanMessage(content=user_input))\n",
    "        else:\n",
    "            state = existing_state.values\n",
    "            state['user_query'].append(user_input)\n",
    "            state['report_state'] = self.report_state  # Ensure it's updated\n",
    "\n",
    "\n",
    "        state['messages'] = add_messages(state['messages'], HumanMessage(content=user_input))\n",
    "\n",
    "        return {\"state\": state, \"run_config\": run_config}\n",
    "\n",
    "    async def predict(self, input_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Run the finqa_graph and return the model output.\n",
    "\n",
    "        Args:\n",
    "            input_data (dict): {'user_id': ..., 'user_input': ...}\n",
    "\n",
    "        Returns:\n",
    "            str: The model's response (message_out)\n",
    "        \"\"\"\n",
    "        if self.df_docs is not None:\n",
    "            globals()[\"df_docs\"] = self.df_docs\n",
    "        \n",
    "        converted = self.convert_input(input_data)\n",
    "        state = converted[\"state\"]\n",
    "        run_config = converted[\"run_config\"]\n",
    "\n",
    "        # Run the graph asynchronously\n",
    "        agent_response = await self.agent.ainvoke(state, config=run_config)\n",
    "\n",
    "        # Retrieve the model output\n",
    "        message_out = agent_response[\"message_out\"]\n",
    "\n",
    "        return {\"output\": message_out,\n",
    "                \"full_response\": agent_response['full_response']\n",
    "        }\n",
    "\n",
    "\n",
    "FinQAAgent = FinQAResponseAgent(finqa_graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd2e64",
   "metadata": {},
   "source": [
    "## Test the Agent\n",
    "\n",
    "### Question:\n",
    "By what percentage did Net Profit After Tax (NPAT) increase from FY23 to FY24?\n",
    "\n",
    "- Answer: NPAT increased by 11.76% from FY23 to FY24.\n",
    "\n",
    "### Question:\n",
    "By how many million dollars did net investment returns rise in FY24, and what is the percentage increase?\n",
    "\n",
    "- Answer: Investment returns rose by A$210m, a 46.56% increase.\n",
    "\n",
    "###  Question:\n",
    "What proportion of cash earnings did net incurred claims represent in FY24?\n",
    "\n",
    "- Answer: Net incurred claims were 361.1% of cash earnings.\n",
    "\n",
    "### Question:\n",
    "If Suncorp paid out 72.1% of its cash earnings as dividends in FY24, what was the total dividend amount?\n",
    "\n",
    "- Answer: Total dividends ≈ A$988.3 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afe7b17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 00:31:12,051 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A continuation Question?  No\n",
      "connected queries: \n",
      "  ['If Suncorp paid out 72.1% of its cash earnings as dividends in FY24, what was the total dividend amount?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 00:31:13,316 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'a': 1372, 'b': 0.721}, 'id': 'fc_cfeca33c-59a7-4bb0-a364-6f0cb537fd0b', 'type': 'tool_call'}]\n",
      "Tool multiply invoked with args {'a': 1372, 'b': 0.721}, got response: 989.212 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 00:31:14,227 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 Agent Response:\\n Thought:  \n",
      "1. The context states that cash earnings for FY24 were $1,372 million.  \n",
      "2. The dividend payout ratio is 72.1% of those cash earnings.  \n",
      "3. Multiply 1,372 million by 0.721 to find the total dividend amount.  \n",
      "4. The tool calculation confirms the result: 1,372 × 0.721 = 989.212 million.\n",
      "\n",
      "References:  \n",
      "- “cash earnings increased 16.6% to $1,372 million.”  \n",
      "- “The Group's full year dividend payout ratio of 72.1% of cash earnings …”\n",
      "\n",
      "Final_answer: The total dividend paid in FY24 was **$989.212 million**.\n",
      "--------------------------------------------------\n",
      "💬 Full messages:\\n content=\"Thought:  \\n1. The context states that cash earnings for FY24 were $1,372\\u202fmillion.  \\n2. The dividend payout ratio is 72.1% of those cash earnings.  \\n3. Multiply 1,372\\u202fmillion by 0.721 to find the total dividend amount.  \\n4. The tool calculation confirms the result: 1,372\\u202f×\\u202f0.721\\u202f=\\u202f989.212\\u202fmillion.\\n\\nReferences:  \\n- “cash earnings increased 16.6% to $1,372 million.”  \\n- “The Group's full year dividend payout ratio of 72.1% of cash earnings …”\\n\\nFinal_answer: The total dividend paid in FY24 was **$989.212\\u202fmillion**.\" additional_kwargs={'reasoning_content': 'We need to answer: \"If Suncorp paid out 72.1% of its cash earnings as dividends in FY24, what was the total dividend amount?\" We have context: cash earnings increased 16.6% to $1,372 million. So cash earnings = $1,372 million. Dividend payout ratio 72.1% of cash earnings. So dividend amount = 0.721 * 1,372 = 989.212 million. The tool multiply gave 989.212. So answer: $989.212 million. Provide references: cash earnings figure, payout ratio. Provide final answer.'} response_metadata={'token_usage': {'completion_tokens': 290, 'prompt_tokens': 2576, 'total_tokens': 2866, 'completion_time': 0.295470346, 'prompt_time': 0.132112978, 'queue_time': 0.422311628, 'total_time': 0.427583324, 'prompt_tokens_details': {'cached_tokens': 256}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_77f8660d1d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--7749aa8e-2615-4731-8077-f0097ac95400-0' usage_metadata={'input_tokens': 2576, 'output_tokens': 290, 'total_tokens': 2866}\n"
     ]
    }
   ],
   "source": [
    "# Test the async agent with sample\n",
    "\n",
    "async def test_agent():\n",
    "    \"\"\"Test the FinQAResponseAgent with a sample input.\"\"\"\n",
    "    FinQAAgent.set_report_state(True)\n",
    "    FinQAAgent.set_documents(df_docs)\n",
    "    input_to_agent = {\"user_id\": \"Carly\", \n",
    "                      \"user_input\": \"If Suncorp paid out 72.1% of its cash earnings as dividends in FY24, what was the total dividend amount?\"}\n",
    "    response = await FinQAAgent.predict(input_to_agent)\n",
    "    print(r\"💬 Agent Response:\\n\", response['output'])\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(r\"💬 Full messages:\\n\", response['full_response'])\n",
    "\n",
    "\n",
    "\n",
    "# Run the async workflow\n",
    "await test_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rmohajer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
